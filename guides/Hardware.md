Resources on deciding what hardware to use for powering your local LLMs.

Relatively maintained resources:

- [Tim Dettmers](https://timdettmers.com/about/) keeps a relatively up-to-date guide of recommendations: [Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/)
- [LLM Utils](https://llm-utils.org/Home) is a documentation resource similar to llm-tracker that has a dedicated subsite [GPU Utils](https://gpus.llm-utils.org/) that has GPU recommendations and requirements for various use-cases. 
- [r/LocalLLaMA Wiki Models](https://www.reddit.com/r/LocalLLaMA/wiki/models/) has a list of memory requirements for different model types, parameters, quantizes

Possibly out of date articles:

- 2023-06-30 MosaicML [compares training with MI250X vs A100 and H100](https://www.mosaicml.com/blog/amd-mi250)
- 2023-06-15 [r/LocalLLaMA: Free GPU options for LlaMA model experimentation](https://www.reddit.com/r/LocalLLaMA/comments/14a0bs9/free_gpu_options_for_llama_model_experimentation/)