- [EvalPlus Leaderboard](https://evalplus.github.io/leaderboard.html)
	- https://github.com/evalplus/evalplus
- [CRUXEval Leaderboard](https://crux-eval.github.io/leaderboard.html)
	- https://github.com/facebookresearch/cruxeval
- [CanAiCode Leaderboard](https://huggingface.co/spaces/mike-ravkine/can-ai-code-results)
	- https://github.com/the-crypt-keeper/can-ai-code
- [Big Code Models Leaderboard](https://huggingface.co/spaces/bigcode/bigcode-models-leaderboard)
- InfiCoder-Eval
	- https://infi-coder.github.io/inficoder-eval/
- [TabbyML Coding LLMs Leaderboard](https://leaderboard.tabbyml.com/)

Running human-eval:
[https://github.com/abacaj/code-eval](https://github.com/abacaj/code-eval)


https://www.reddit.com/r/LocalLLaMA/comments/18m54tw/real_world_multi_step_reasoning_software/
https://galois.com/blog/2023/09/using-gpt-4-to-assist-in-c-to-rust-translation/
https://galois.com/blog/2023/08/applying-gpt-4-to-saw-formal-verification/
https://twitter.com/a_karvonen/status/1717168110505955568


- [Zero-Shot Replication Framework](https://github.com/emrgnt-cmplxty/zero-shot-replication) - replicate HumanEval, LeetCodeSparks, LeetCode100
- [code-eval](https://github.com/abacaj/code-eval) - scripts for running/reproducing human-eval scores on models
- [llm-humaneval-benchmarks](https://github.com/my-other-github-account/llm-humaneval-benchmarks) - HuggingFace models evald vs HumanEval+
- Multilingual Code Models Evaluation - base multilingual code generation models
- [airate](https://github.com/catid/supercharger/tree/main/airate) - C++ bug catching test
- [phi-1 prompt tests](https://twitter.com/khandelia1000/status/1675939866389934097)
    - [https://colab.research.google.com/drive/1mSb2t8NDz0o\_Cc8VgTMbhOg8kIh-cRIu?usp=sharing](https://colab.research.google.com/drive/1mSb2t8NDz0o_Cc8VgTMbhOg8kIh-cRIu?usp=sharing)
