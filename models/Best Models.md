
| Last Reviewed | Class of Model | Model | Notes |
| ---- | ---- | ---- | ---- |
| 2024-01-28 | 3B | [stabilityai/stablelm-zephyr-3b](https://huggingface.co/stabilityai/stablelm-zephyr-3b) | 6.64 MT-Bench<br>Stability NC License |
| 2024-01-28 | 7B | [mlabonne/OmniBeagle-7B](https://huggingface.co/mlabonne/OmniBeagle-7B) | Highest Nous Suite<br>CC-BY-NC 4.0<br>Big merge<br>7.96 MT-Bench  |
| 2024-01-18 | 13B | [NousResearch/Nous-Hermes-2-SOLAR-10.7B](https://huggingface.co/NousResearch/Nous-Hermes-2-SOLAR-10.7B) | Apache 2.0<br>Hermes tune of SOLAR (layer stacked Mistral 7B) |
| 2024-01-18 | 4x7B MoE | [mlabonne/Beyonder-4x7B-v2](https://huggingface.co/mlabonne/Beyonder-4x7B-v2) | MS Research License / CC-BY-NC 4.0<br>A very capable looking model that has 24.2B parameters, but inferences at ~12B (2 experts) |
| 2024-01-18 | 34B | [jondurbin/bagel-dpo-34b-v0.2](https://huggingface.co/jondurbin/bagel-dpo-34b-v0.2) | Yi License (Instant Commercial)<br>[Reddit Discussion](https://www.reddit.com/r/LocalLLaMA/comments/18w8hfw/bagel_34b_dpo_yi_200k_finetuned_on_everything/)<br>Yi 200K based finetune |
| 2024-01-18 | 8x7B | [NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO) |  |
| 2024-01-18 | 70B+ | [miqudev/miqu-1-70b](https://huggingface.co/miqudev/miqu-1-70b) | Leaked alpha of mistral-medium<br>fp16 dequant here: [152334H/miqu-1-70b-sf](https://huggingface.co/152334H/miqu-1-70b-sf)<br>See also this test for the best 70B-120B models |
| 2024-01-19 | Coding | [deepseek-ai/deepseek-coder-33b-instruct](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct) | There is also a [v1.5 of the 7b just released](https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct-v1.5), but if you have the resources the 33B is going to reason better (recent test shows [even the v1.0 7b to be great](https://www.reddit.com/r/LocalLLaMA/comments/19fc4uf/baseline_benchmark_for_17_coding_models/)) |
