| Last Reviewed | Class of Model | Model | Notes |
| ---- | ---- | ---- | ---- |
| 2024-01-28 | 3B | [stabilityai/stablelm-zephyr-3b](https://huggingface.co/stabilityai/stablelm-zephyr-3b) | 6.64 MT-Bench<br>Stability NC License |
| 2024-01-28 | 7B | [mlabonne/NeuralBeagle14-7B](https://huggingface.co/mlabonne/NeuralBeagle14-7B) | Highest Nous Suite<br>CC-BY-NC 4.0<br>Merge (UNA Beagle, Marcoro14 slerp)<br>There's also a 10.7B stacked [FrankenBeagle14-11B](https://huggingface.co/mlabonne/FrankenBeagle14-11B) version |
| 2024-01-18 | 13B | [NousResearch/Nous-Hermes-2-SOLAR-10.7B](https://huggingface.co/NousResearch/Nous-Hermes-2-SOLAR-10.7B) | Apache 2.0<br>Hermes tune of SOLAR (layer stacked Mistral 7B) |
| 2024-01-18 | 4x7B MoE | [mlabonne/Beyonder-4x7B-v2](https://huggingface.co/mlabonne/Beyonder-4x7B-v2) | MS Research License / CC-BY-NC 4.0<br>A very capable looking model that has 24.2B parameters, but inferences at ~12B (2 experts) |
| 2024-01-18 | 34B | [jondurbin/bagel-dpo-34b-v0.2](https://huggingface.co/jondurbin/bagel-dpo-34b-v0.2) | Yi License (Instant Commercial)<br>[Reddit Discussion](https://www.reddit.com/r/LocalLLaMA/comments/18w8hfw/bagel_34b_dpo_yi_200k_finetuned_on_everything/)<br>Yi 200K based finetune |
| 2024-01-18 | 8x7B | [NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO) |  |
