
| Last Reviewed | Class of Model | Model                                                                                                             | Notes                                                                                                                                                                                                                                                                                                                                 |
| ------------- | -------------- | ----------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 2024-01-28    | 3B             | [stabilityai/stablelm-zephyr-3b](https://huggingface.co/stabilityai/stablelm-zephyr-3b)                           | 6.64 MT-Bench<br>Stability NC License                                                                                                                                                                                                                                                                                                 |
| 2024-02-25    | 7B             | [mlabonne/AlphaMonarch-7B](https://huggingface.co/mlabonne/AlphaMonarch-7B)                                       | Current endpoint of mlabonne's merging experiments.<br>High performing across wide range of benchmarks<br>CC-BY-NC 4.0                                                                                                                                                                                                                |
| 2024-01-18    | 13B            | [NousResearch/Nous-Hermes-2-SOLAR-10.7B](https://huggingface.co/NousResearch/Nous-Hermes-2-SOLAR-10.7B)           | Apache 2.0<br>Hermes tune of SOLAR (layer stacked Mistral 7B)                                                                                                                                                                                                                                                                         |
| 2024-01-18    | 4x7B MoE       | [mlabonne/Beyonder-4x7B-v2](https://huggingface.co/mlabonne/Beyonder-4x7B-v2)                                     | MS Research License / CC-BY-NC 4.0<br>A very capable looking model that has 24.2B parameters, but inferences at ~12B (2 experts)                                                                                                                                                                                                      |
| 2024-01-18    | 34B            | [jondurbin/bagel-dpo-34b-v0.2](https://huggingface.co/jondurbin/bagel-dpo-34b-v0.2)                               | Yi License (Instant Commercial)<br>[Reddit Discussion](https://www.reddit.com/r/LocalLLaMA/comments/18w8hfw/bagel_34b_dpo_yi_200k_finetuned_on_everything/)<br>Yi 200K based finetune                                                                                                                                                 |
| 2024-01-18    | 8x7B           | [NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO) | Best performing Mixtral?                                                                                                                                                                                                                                                                                                              |
| 2024-01-18    | 70B+           | [miqudev/miqu-1-70b](https://huggingface.co/miqudev/miqu-1-70b)                                                   | Leaked alpha of mistral-medium<br>fp16 dequant here: [152334H/miqu-1-70b-sf](https://huggingface.co/152334H/miqu-1-70b-sf)<br>See also this [test of various miqu models](https://www.reddit.com/r/LocalLLaMA/comments/1aix93e/llm_comparisontest_miqu_miqu_miqu_miquella_maid/)                                                      |
| 2024-02-25    | 70B+           | [abacusai/Smaug-72B-v0.1](https://huggingface.co/abacusai/Smaug-72B-v0.1)                                         | A DPOP tune of Qwen-72B (1.0) - topped the HF leaderboards and scores >80 on MMLU, however haven't seen much real world reporting. Note: Qwen                                                                                                                                                                                         |
|               |                |                                                                                                                   |                                                                                                                                                                                                                                                                                                                                       |
| 2024-01-19    | Coding         | [deepseek-ai/deepseek-coder-33b-instruct](https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct)         | There is also a [v1.5 of the 7b just released](https://huggingface.co/deepseek-ai/deepseek-coder-7b-instruct-v1.5), but if you have the resources the 33B is going to reason better (recent test shows [even the v1.0 7b to be great](https://www.reddit.com/r/LocalLLaMA/comments/19fc4uf/baseline_benchmark_for_17_coding_models/)) |
