# Trainers
* LLaMA-Factory
* XTuner

7B Fine Tune: https://huggingface.co/deepseek-ai/deepseek-llm-7b-base

67B Fine Tune:
https://huggingface.co/deepseek-ai/deepseek-llm-67b-base
https://github.com/deepseek-ai/DeepSeek-LLM

vs Mixtral

DeepSeek 67B vs Mixtral 8x7B - chatntq qlora

KTO
https://contextual.ai/better-cheaper-faster-llm-alignment-with-kto/
https://github.com/ContextualAI/HALOs/blob/main/assets/report.pdf

MergeKit
https://github.com/cg123/mergekit/blob/mixtral/moe.md

Test
https://huggingface.co/openchat/openchat-3.5-1210
https://github.com/imoneoi/openchat
https://openchat.team/


# Magicoder on DeepSeek-Coder 33B, CodeLlama 70B
Base Models
- https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct
- https://huggingface.co/codellama/CodeLlama-70b-hf
Info:
- https://github.com/ise-uiuc/magicoder/
- https://huggingface.co/ise-uiuc/Magicoder-S-DS-6.7B
	- https://huggingface.co/datasets/ise-uiuc/Magicoder-OSS-Instruct-75K
	- https://huggingface.co/datasets/ise-uiuc/Magicoder-Evol-Instruct-110K